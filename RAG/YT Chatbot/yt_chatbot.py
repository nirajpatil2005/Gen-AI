# -*- coding: utf-8 -*-
"""YT-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vch16l3kUSL0SWG_F2DuyoUJCf5ovr-
"""

!pip install langchain_groq

!pip install -q youtube-transcript-api langchain-community langchain-openai \
               faiss-cpu tiktoken python-dotenv

pip install langchain_huggingface langchain_community langchain_text_splitters

from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint
from dotenv import load_dotenv
import os
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
load_dotenv()

hf_token=os.getenv("HF_TOKEN")

llm = HuggingFaceEndpoint(
    repo_id="moonshotai/Kimi-K2-Instruct",  # Use the correct model repo_id
    task="text-generation",  # You can adjust this based on your model task
    model_kwargs={"headers": {"Authorization": f"Bearer {hf_token}"}}
)

model=ChatHuggingFace(llm=llm)

from langchain_groq import ChatGroq
import os

groq_token = os.getenv("test_groq")
llm = ChatGroq(
    groq_api_key=groq_token,
    model_name="meta-llama/llama-4-scout-17b-16e-instruct"
)

# Use llm directly - no need for ChatHuggingFace wrapper
response = llm.invoke("Your prompt here")

from youtube_transcript_api import YouTubeTranscriptApi

api = YouTubeTranscriptApi()

# List available transcripts with types, dialects, and translation support
transcript_list = api.list("Gfr50f6ZBvo")

for t in transcript_list:
    print(
        f"- {t.language_code} "
        f"({'manual' if not t.is_generated else 'auto-generated'}) "
        f"{'[translatable]' if t.is_translatable else ''}"
    )

from youtube_transcript_api import YouTubeTranscriptApi
print(dir(YouTubeTranscriptApi))  # Should list 'get_transcript'

from youtube_transcript_api import YouTubeTranscriptApi

video_id = "Gfr50f6ZBvo"  # Example video

# Create an instance first
api = YouTubeTranscriptApi()

# Now call fetch()
transcript = api.fetch(video_id, languages=['en'])
print(transcript)

# Assuming 'transcript' is your FetchedTranscript object
transcript_dict_list = [
    {
        "text": snippet.text,
        "start": snippet.start,
        "duration": snippet.duration
    }
    for snippet in transcript.snippets
]

# Print the first 5 entries to verify
print(transcript_dict_list[:5])

import json
json_output = json.dumps(transcript_dict_list, indent=2)
print(json_output)

"""# 1.**Indexing** (Text Splitting)"""

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Extract just the text from your transcript
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # Focused paragraphs
    chunk_overlap=600,  # Preserves context
    separators=["\n\n", "\n", ". ", "? ", "! "]  # Split at natural boundaries
)

chunks = splitter.create_documents(texts)

len(chunks)

chunks[0]

"""# **1.Indexing (Embedding Generation and storing in vector store)**"""

from langchain_community.embeddings import HuggingFaceEmbeddings

# Lightweight model (good for CPU)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Higher quality (requires more RAM)
# embeddings = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")

vector_store=FAISS.from_documents(chunks,embedding=embeddings)

vector_store.index_to_docstore_id

vector_store.get_by_ids(['d87c9b26-c4c2-48de-9f68-40a4793677e0'])

"""# **2.Retrieval**"""

retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={
        'k': 8,  # More documents
        'lambda_mult': 0.6,  # Balanced diversity
        'fetch_k': 20  # Larger candidate pool
    }
)

retriever

retriever.invoke('what is deepmind')

"""# **3.Augmentation**"""

llm

prompt = PromptTemplate(
    template="""
    Analyze this context thoroughly:
    {context}

    Question: {question}

    Guidelines:
    1. If the name appears multiple times, identify all relevant positions
    2. Connect related information across chunks
    3. State if information is incomplete
    """,
    input_variables=['context', 'question']
)

question="is the topic of aliens discussed in this video ? if yes then what was discussed?"
retrieved_docs=retriever.invoke(question)

context_text=f" \n\n ".join(doc.page_content for doc in retrieved_docs)

final_prompt=prompt.invoke({"context":context_text,"question":question})

"""# **4.Generation**"""

retrieved_docs

context_text

final_prompt

answer=llm.invoke(final_prompt)
print(answer)

print(answer.content)

"""# 5.Building a **Chain**"""

from langchain_core.runnables import RunnableParallel,RunnableSequence,RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

def format_docs(retrieved_docs):
  context_text="\n\n".join(doc.page_content for doc in retrieved_docs)
  return context_text

parallel_chain=RunnableParallel({
    'context':retriever | RunnableLambda(format_docs),
    'question':RunnablePassthrough()
})

parallel_chain.invoke('who is demis')

parser=StrOutputParser()

main_chain=parallel_chain | prompt| llm |parser

main_chain.invoke("Can you summarize the video")

