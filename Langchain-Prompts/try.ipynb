{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 00:58:04.443 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\niraj\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-07-20 00:58:04.443 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import langchain_huggingface\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "st.header(\"Research Tool\")\n",
    "user_input=st.text_input('Enter your prompt')\n",
    "\n",
    "if st.button:\n",
    "    st.text('Some random text')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c150eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about Langchain', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is an open-source framework (originally in Python, now also in TypeScript) that makes it dramatically easier to build applications powered by large-language-models (LLMs).  Instead of treating an LLM as a black-box text generator, LangChain gives you a set of composable “building blocks” that let you:\\n\\n1. Connect the LLM to external data (documents, APIs, databases, the web).\\n2. Chain together multiple LLM calls, prompts, and other logic.\\n3. Remember context across turns (conversation memory).\\n4. Observe, debug, and evaluate what the model is doing.\\n\\nThink of it as “Rails for LLM apps” or “scikit-learn for prompt engineering.”\\n\\n────────────────────────────────────────\\nCore Concepts\\n────────────────────────────────────────\\n1. Models  \\n   • Chat models (OpenAI GPT-4, Anthropic Claude, Llama-2, etc.)  \\n   • Embedding models (OpenAI text-embedding-ada-002, Sentence-Transformers, etc.)  \\n   • LLMs (legacy text-completion endpoints)\\n\\n2. Prompt Templates  \\n   Re-usable, parameterizable prompts with Jinja2/F-string style templating.\\n\\n3. Output Parsers  \\n   Automatically coerce free-form text into Pydantic objects, JSON, lists, dates, etc.\\n\\n4. Indexes & Retrievers  \\n   • Loaders: PDF, CSV, Notion, Google Drive, web scraping, SQL, etc.  \\n   • Text splitters: chunk by tokens, characters, or semantic boundaries.  \\n   • Vector stores: FAISS, Pinecone, Weaviate, Chroma, PGVector, ElasticSearch.  \\n   • Retrievers: similarity search, MMR, parent-doc, multi-query, ensemble.\\n\\n5. Chains  \\n   A “chain” is a sequence of calls—LLM or otherwise—wired together.  \\n   • LLMChain: simplest (prompt → model → parse).  \\n   • ConversationalRetrievalChain: chat over docs with memory.  \\n   • RetrievalQA, SQLDatabaseChain, APIChain, Map-Reduce, Refine, etc.\\n\\n6. Memory  \\n   Store and inject conversation history, summary buffers, entity memory, or custom stores (Redis, DynamoDB, Postgres).\\n\\n7. Agents & Toolkits  \\n   Give the LLM access to tools (search, calculator, Python REPL, Zapier, SQL, etc.) and let it decide which tool to call and in what order.  \\n   • Zero-shot ReAct, OpenAI Functions, Plan-and-Execute, Self-Ask, conversational agents.\\n\\n8. Callbacks & Observability  \\n   Streaming, logging, tracing (LangSmith), token counting, latency metrics.\\n\\n────────────────────────────────────────\\nTypical Use-Case Patterns\\n────────────────────────────────────────\\n• “Chat with your PDF” – load PDF → split → embed → store in vector DB → retrieval-augmented chat.  \\n• SQL or CSV analyst – natural language → SQL query → execute → summarize results.  \\n• Multi-step research agent – search web → scrape → summarize → answer.  \\n• Code interpreter – LLM writes Python → sandbox executes → returns plots.  \\n• Personal assistant – remembers user facts, schedules via Zapier, drafts emails.\\n\\n────────────────────────────────────────\\nEcosystem & Tooling\\n────────────────────────────────────────\\n• LangSmith – hosted debugging, testing, and production monitoring.  \\n• LangServe – turn any chain into a REST API with FastAPI.  \\n• LangGraph – build cyclic, stateful multi-actor workflows (DAGs + loops).  \\n• LangChain Templates – pre-built reference architectures you can clone.\\n\\n────────────────────────────────────────\\nMinimal Python Example\\n────────────────────────────────────────\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.schema import SystemMessage, HumanMessage\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chains import LLMChain\\n\\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a helpful assistant.\"),\\n    (\"human\", \"{question}\")\\n])\\n\\nchain = LLMChain(llm=llm, prompt=prompt)\\nprint(chain.run(question=\"Explain LangChain in one sentence.\"))\\n\\n────────────────────────────────────────\\nWhen NOT to use LangChain\\n────────────────────────────────────────\\n• Ultra-simple single-shot prompts (OpenAI SDK is lighter).  \\n• Latency-critical streaming where you need bare-metal control.  \\n• Non-Python/TypeScript stacks (though ports exist: LangChain4j, LangChain.rb, LangChainGo).\\n\\n────────────────────────────────────────\\nStatus & Governance\\n────────────────────────────────────────\\nLangChain Inc. (the company) was founded by Harrison Chase in 2023; the framework itself is MIT-licensed and community-driven.  Releases are frequent (weekly), so pin versions in production.\\n\\n────────────────────────────────────────\\nOne-Sentence Summary\\n────────────────────────────────────────\\nLangChain is the de-facto standard toolkit that turns raw LLMs into production-grade, data-aware, agentic applications by giving developers modular primitives for prompts, memory, retrieval, and orchestration.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate,load_prompt\n",
    "\n",
    "from langchain_core.messages import SystemMessage,HumanMessage,AIMessage\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch the Hugging Face token from the environment variable\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Set up the Hugging Face model endpoint\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"moonshotai/Kimi-K2-Instruct\",  # Use the correct model repo_id\n",
    "    task=\"text-generation\",  # You can adjust this based on your model task\n",
    "    model_kwargs={\"headers\": {\"Authorization\": f\"Bearer {hf_token}\"}}\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Tell me about Langchain\")\n",
    "]\n",
    "\n",
    "result=model.invoke(messages)\n",
    "\n",
    "messages.append(AIMessage(content=result.content))\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476cc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
